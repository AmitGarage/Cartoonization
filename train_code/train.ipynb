{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "agreed-pledge",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'network'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2-c49148b051c4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0margparse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mnetwork\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'network'"
     ]
    }
   ],
   "source": [
    "'''\n",
    "Source code for CVPR 2020 paper\n",
    "'Learning to Cartoonize Using White-Box Cartoon Representations'\n",
    "by Xinrui Wang and Jinze yu\n",
    "'''\n",
    "\n",
    "\n",
    "import torch\n",
    "\n",
    "import utils\n",
    "import os\n",
    "import numpy as np\n",
    "import argparse\n",
    "import network \n",
    "import loss\n",
    "\n",
    "from tqdm import tqdm\n",
    "from guided_filter import guided_filter\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "alien-tomato",
   "metadata": {},
   "outputs": [],
   "source": [
    "def arg_parser():\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument(\"--patch_size\", default = 256, type = int)\n",
    "    parser.add_argument(\"--batch_size\", default = 16, type = int)     \n",
    "    parser.add_argument(\"--total_iter\", default = 100000, type = int)\n",
    "    parser.add_argument(\"--adv_train_lr\", default = 2e-4, type = float)\n",
    "    parser.add_argument(\"--gpu_fraction\", default = 0.5, type = float)\n",
    "    parser.add_argument(\"--save_dir\", default = 'train_cartoon', type = str)\n",
    "    parser.add_argument(\"--use_enhance\", default = False)\n",
    "\n",
    "    args = parser.parse_args()\n",
    "    \n",
    "    return args"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "possible-springer",
   "metadata": {},
   "outputs": [],
   "source": [
    "def initalization(path_to_previous_weight):\n",
    "    vgg_model = loss.vgg19beforefc(path_to_previous_weight)\n",
    "    g_optim = torch.optim.Adam(vgg_model.parameters(),args.adv_train_lr, beta1=0.5, beta2=0.99)\n",
    "    d_optim = torch.optim.Adam(vgg_model.parameters(),args.adv_train_lr, beta1=0.5, beta2=0.99)\n",
    "    \n",
    "    return vgg_model,g_optim,d_optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "built-endorsement",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(args,vgg_model,g_optim,d_optim):\n",
    "    \n",
    "    #input_photo = tf.placeholder(tf.float32, [args.batch_size,args.patch_size, args.patch_size, 3])\n",
    "    #input_superpixel = tf.placeholder(tf.float32, [args.batch_size,args.patch_size, args.patch_size, 3])\n",
    "    #input_cartoon = tf.placeholder(tf.float32, [args.batch_size,args.patch_size, args.patch_size, 3])\n",
    "    \n",
    "    #output = network.unet_generator(input_photo)\n",
    "    #output = guided_filter(input_photo, output, r=1)\n",
    "\n",
    "    \n",
    "    #blur_fake = guided_filter(output, output, r=5, eps=2e-1)\n",
    "    #blur_cartoon = guided_filter(input_cartoon, input_cartoon, r=5, eps=2e-1)\n",
    "\n",
    "    #gray_fake, gray_cartoon = utils.color_shift(output, input_cartoon)\n",
    "    \n",
    "    #d_loss_gray, g_loss_gray = loss.lsgan_loss(network.disc_sn, gray_cartoon, gray_fake,scale=1, patch=True, name='disc_gray')\n",
    "    #d_loss_blur, g_loss_blur = loss.lsgan_loss(network.disc_sn, blur_cartoon, blur_fake,scale=1, patch=True, name='disc_blur')\n",
    "\n",
    "\n",
    "    #vgg_model = loss.Vgg19('vgg19_no_fc.npy')\n",
    "    #vgg_photo = vgg_model.build_conv4_4(input_photo)\n",
    "    #vgg_output = vgg_model.build_conv4_4(output)\n",
    "    #vgg_superpixel = vgg_model.build_conv4_4(input_superpixel)\n",
    "    #h, w, c = vgg_photo.get_shape().as_list()[1:]\n",
    "    \n",
    "    #photo_loss = tf.reduce_mean(tf.losses.absolute_difference(vgg_photo, vgg_output))/(h*w*c)\n",
    "    #superpixel_loss = tf.reduce_mean(tf.losses.absolute_difference(vgg_superpixel, vgg_output))/(h*w*c)\n",
    "    #recon_loss = photo_loss + superpixel_loss\n",
    "    #tv_loss = loss.total_variation_loss(output)\n",
    "    \n",
    "    #g_loss_total = 1e4*tv_loss + 1e-1*g_loss_blur + g_loss_gray + 2e2*recon_loss\n",
    "    #d_loss_total = d_loss_blur + d_loss_gray\n",
    "\n",
    "    #all_vars = tf.trainable_variables()\n",
    "    #gene_vars = [var for var in all_vars if 'gene' in var.name]\n",
    "    #disc_vars = [var for var in all_vars if 'disc' in var.name] \n",
    "    \n",
    "    \n",
    "    #tf.summary.scalar('tv_loss', tv_loss)\n",
    "    #tf.summary.scalar('photo_loss', photo_loss)\n",
    "    #tf.summary.scalar('superpixel_loss', superpixel_loss)\n",
    "    #tf.summary.scalar('recon_loss', recon_loss)\n",
    "    #tf.summary.scalar('d_loss_gray', d_loss_gray)\n",
    "    #tf.summary.scalar('g_loss_gray', g_loss_gray)\n",
    "    #tf.summary.scalar('d_loss_blur', d_loss_blur)\n",
    "    #tf.summary.scalar('g_loss_blur', g_loss_blur)\n",
    "    #tf.summary.scalar('d_loss_total', d_loss_total)\n",
    "    #tf.summary.scalar('g_loss_total', g_loss_total)\n",
    "      \n",
    "    #update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    #with tf.control_dependencies(update_ops):\n",
    "        \n",
    "        #g_optim = tf.train.AdamOptimizer(args.adv_train_lr, beta1=0.5, beta2=0.99).minimize(g_loss_total, var_list=gene_vars)\n",
    "        \n",
    "        #d_optim = tf.train.AdamOptimizer(args.adv_train_lr, beta1=0.5, beta2=0.99).minimize(d_loss_total, var_list=disc_vars)\n",
    "           \n",
    "    '''\n",
    "    config = tf.ConfigProto()\n",
    "    config.gpu_options.allow_growth = True\n",
    "    sess = tf.Session(config=config)\n",
    "    '''\n",
    "    #gpu_options = tf.GPUOptions(per_process_gpu_memory_fraction=args.gpu_fraction)\n",
    "    #sess = tf.Session(config=tf.ConfigProto(gpu_options=gpu_options))\n",
    "    \n",
    "    \n",
    "    #train_writer = tf.summary.FileWriter(args.save_dir+'/train_log')\n",
    "    #summary_op = tf.summary.merge_all()\n",
    "    #saver = tf.train.Saver(var_list=gene_vars, max_to_keep=20)\n",
    "   \n",
    "    #with tf.device('/device:GPU:0'):\n",
    "    \n",
    "    device_to_use = ('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    latest_checkpoint = torch.load('/content/drive/MyDrive/model_weights.pt',map_location=torch.device(device_to_use))\n",
    "\n",
    "    face_photo_dir = 'dataset/photo_face'\n",
    "    face_photo_list = utils.load_image_list(face_photo_dir)\n",
    "    scenery_photo_dir = 'dataset/photo_scenery'\n",
    "    scenery_photo_list = utils.load_image_list(scenery_photo_dir)\n",
    "\n",
    "    face_cartoon_dir = 'dataset/cartoon_face'\n",
    "    face_cartoon_list = utils.load_image_list(face_cartoon_dir)\n",
    "    scenery_cartoon_dir = 'dataset/cartoon_scenery'\n",
    "    scenery_cartoon_list = utils.load_image_list(scenery_cartoon_dir)\n",
    "\n",
    "    writer = SummaryWriter()\n",
    "    \n",
    "    for total_iter in tqdm(range(args.total_iter)):\n",
    "\n",
    "        if np.mod(total_iter, 5) == 0: \n",
    "            photo_batch = utils.next_batch(face_photo_list, args.batch_size)\n",
    "            cartoon_batch = utils.next_batch(face_cartoon_list, args.batch_size)\n",
    "        else:\n",
    "            photo_batch = utils.next_batch(scenery_photo_list, args.batch_size)\n",
    "            cartoon_batch = utils.next_batch(scenery_cartoon_list, args.batch_size)\n",
    "        \n",
    "        #inter_out = sess.run(output, feed_dict={input_photo: photo_batch,input_superpixel: photo_batch,input_cartoon: cartoon_batch})\n",
    "        \n",
    "        g_optim.zero_grad()\n",
    "        d_optim.zero_grad()\n",
    "        \n",
    "        output = network.unet_generator(photo_batch)\n",
    "        output = guided_filter(photo_batch, output, r=1)\n",
    "        \n",
    "        inter_out = output\n",
    "        \n",
    "        blur_fake = guided_filter(output, output, r=5, eps=2e-1)\n",
    "        blur_cartoon = guided_filter(cartoon_batch, cartoon_batch, r=5, eps=2e-1)\n",
    "\n",
    "        gray_fake, gray_cartoon = utils.color_shift(output, cartoon_batch)\n",
    "\n",
    "        '''\n",
    "        adaptive coloring has to be applied with the clip_by_value \n",
    "        in the last layer of generator network, which is not very stable.\n",
    "        to stabiliy reproduce our results, please use power=1.0\n",
    "        and comment the clip_by_value function in the network.py first\n",
    "        If this works, then try to use adaptive color with clip_by_value.\n",
    "        '''\n",
    "            \n",
    "        if args.use_enhance:\n",
    "            superpixel_batch = utils.selective_adacolor(inter_out, power=1.2)\n",
    "        else:\n",
    "            superpixel_batch = utils.simple_superpixel(inter_out, seg_num=200)\n",
    "                \n",
    "        #_, g_loss, r_loss = sess.run([g_optim, g_loss_total, recon_loss],feed_dict={input_photo: photo_batch,input_superpixel: superpixel_batch,input_cartoon: cartoon_batch})\n",
    "        \n",
    "\n",
    "        #_, d_loss, train_info = sess.run([d_optim, d_loss_total, summary_op],feed_dict={input_photo: photo_batch,input_superpixel: superpixel_batch,input_cartoon: cartoon_batch})\n",
    "        \n",
    "        d_loss_gray, g_loss_gray = loss.lsgan_loss(network.disc_sn, gray_cartoon, gray_fake, scale=1, patch=True)\n",
    "        d_loss_blur, g_loss_blur = loss.lsgan_loss(network.disc_sn, blur_cartoon, blur_fake, scale=1, patch=True)\n",
    "        \n",
    "        g_optim.zero_grad()\n",
    "        d_optim.zero_grad()\n",
    "        \n",
    "        vgg_photo = vgg_model.build_model(photo_batch)\n",
    "        vgg_output = vgg_model.build_model(output)\n",
    "        vgg_superpixel = vgg_model.build_model(superpixel_batch)\n",
    "        h, w, c = vgg_photo.get_shape().as_list()[1:]\n",
    "        \n",
    "        main_loss = torch.nn.L1Loss()\n",
    "        photo_loss = torch.mean(main_loss(vgg_photo, vgg_output))/(h*w*c)\n",
    "        superpixel_loss = torch.mean(main_loss(vgg_superpixel, vgg_output))/(h*w*c)\n",
    "        recon_loss = photo_loss + superpixel_loss\n",
    "        tv_loss = loss.total_variation_loss(output)\n",
    "    \n",
    "        g_loss_total = 1e4*tv_loss + 1e-1*g_loss_blur + g_loss_gray + 2e2*recon_loss\n",
    "        d_loss_total = d_loss_blur + d_loss_gray\n",
    "        \n",
    "        g_loss = g_loss_total\n",
    "        r_loss = recon_loss\n",
    "        d_loss = d_loss_total\n",
    "        \n",
    "        g_loss.backward()\n",
    "        d_loss.backward()\n",
    "        \n",
    "        writer.add_scalar('tv_loss', tv_loss, total_iter)\n",
    "        writer.add_scalar('photo_loss', photo_loss, total_iter)\n",
    "        writer.add_scalar('superpixel_loss', superpixel_loss, total_iter)\n",
    "        writer.add_scalar('recon_loss', recon_loss, total_iter)\n",
    "        writer.add_scalar('d_loss_gray', d_loss_gray, total_iter)\n",
    "        writer.add_scalar('g_loss_gray', g_loss_gray, total_iter)\n",
    "        writer.add_scalar('d_loss_blur', d_loss_blur, total_iter)\n",
    "        writer.add_scalar('g_loss_blur', g_loss_blur, total_iter)\n",
    "        writer.add_scalar('d_loss_total', d_loss_total, total_iter)\n",
    "        writer.add_scalar('g_loss_total', g_loss_total, total_iter)\n",
    "            \n",
    "            if np.mod(total_iter+1, 50) == 0:\n",
    "\n",
    "                print('Iter: {}, d_loss: {}, g_loss: {}, recon_loss: {}'.\\\n",
    "                        format(total_iter, d_loss, g_loss, r_loss))\n",
    "                if np.mod(total_iter+1, 500 ) == 0:\n",
    "                    saver.save(sess, args.save_dir+'/saved_models/model', \n",
    "                               write_meta_graph=False, global_step=total_iter)\n",
    "                     \n",
    "                    photo_face = utils.next_batch(face_photo_list, args.batch_size)\n",
    "                    cartoon_face = utils.next_batch(face_cartoon_list, args.batch_size)\n",
    "                    photo_scenery = utils.next_batch(scenery_photo_list, args.batch_size)\n",
    "                    cartoon_scenery = utils.next_batch(scenery_cartoon_list, args.batch_size)\n",
    "\n",
    "                    #result_face = sess.run(output, feed_dict={input_photo: photo_face,input_superpixel: photo_face,input_cartoon: cartoon_face})\n",
    "                    \n",
    "                    output = network.unet_generator(photo_face)\n",
    "                    output = guided_filter(photo_face, output, r=1)\n",
    "                    result_face = output\n",
    "                    \n",
    "                    #result_scenery = sess.run(output, feed_dict={input_photo: photo_scenery,input_superpixel: photo_scenery,input_cartoon: cartoon_scenery})\n",
    "\n",
    "                    output = network.unet_generator(photo_scenery)\n",
    "                    output = guided_filter(photo_scenery, output, r=1)\n",
    "                    result_face = output\n",
    "                    \n",
    "                    utils.write_batch_image(result_face, args.save_dir+'/images',str(total_iter)+'_face_result.jpg', 4)\n",
    "                    utils.write_batch_image(photo_face, args.save_dir+'/images',str(total_iter)+'_face_photo.jpg', 4)\n",
    "\n",
    "                    utils.write_batch_image(result_scenery, args.save_dir+'/images',str(total_iter)+'_scenery_result.jpg', 4)\n",
    "                    utils.write_batch_image(photo_scenery, args.save_dir+'/images',str(total_iter)+'_scenery_photo.jpg', 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "patient-russell",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "usage: ipykernel_launcher.py [-h] [--patch_size PATCH_SIZE]\n",
      "ipykernel_launcher.py: error: unrecognized arguments: -f /root/.local/share/jupyter/runtime/kernel-28da4ab8-bff3-4a50-96aa-0f1e2910d246.json\n"
     ]
    },
    {
     "ename": "SystemExit",
     "evalue": "2",
     "output_type": "error",
     "traceback": [
      "An exception has occurred, use %tb to see the full traceback.\n",
      "\u001b[0;31mSystemExit\u001b[0m\u001b[0;31m:\u001b[0m 2\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    \n",
    "    args = arg_parser()\n",
    "    train(args)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "artistic-sequence",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
