# White Box Cartoonization 
Converting real world images to cartoonized images using White Box Cartoonization algorithm

# Reference to original paper and code ( Tensorflow V1 ) by the creator
[CVPR2020]Learning to Cartoonize Using White-box Cartoon Representations
[project page](https://systemerrorwang.github.io/White-box-Cartoonization/) |   [paper](https://github.com/SystemErrorWang/White-box-Cartoonization/blob/master/paper/06791.pdf) |   [twitter](https://twitter.com/IlIIlIIIllIllII/status/1243108510423896065) |   [zhihu](https://zhuanlan.zhihu.com/p/117422157) |   [bilibili](https://www.bilibili.com/video/av56708333) |  [facial model](https://github.com/SystemErrorWang/FacialCartoonization)

Identify three white-box representations from images :-
- Surface representation - Contains a smooth surface of cartoon images. Extract a weighted low-frequency component, where the color composition and surface texture are preserved with edges, textures and details ignored. Motivated by the cartoon painting behavior where artists usually draw composition drafts before the details are retouched. In this work, we adapt a differentiable guided filter to extract smooth, cartoon-like surface from images, enabling our model to learn structure-level composition and smooth surface that artists have created in cartoon artworks. Lsurface(G, Ds) = logDs(Fdgf (Ic, Ic)) + log(1 − Ds(Fdgf (G(Ip), G(Ip)))). differentiable guided filter Fdgf.
- Structure representation - It refers to the sparse color-blocks and flatten global content in the celluloid style workflow. Extract a segmentation map from the input image and then apply an adaptive coloring algorithm ( Effectively enhances the contrast of images and reduces hazing effect ) on each segmented regions to generate the structure representation. Motivated to emulate the celluloid cartoon style, which is featured by clear boundaries and sparse color blocks. Super-pixel segmentation groups spatially connected pixels in an image with similar color or gray level. In this work, we follow the felzenszwalb algorithm to develop a cartoon-oriented segmentation method to achieve a learnable structure representation. Superpixel algorithms only consider the similarity of pixels and ignore semantic information, we further introduce selective search to merge segmented regions and extract a sparse segmentation map. Lstructure = ||VGGn(G(Ip)) − VGGn(Fst(G(Ip)))||
- Texture representation - It reflects highfrequency texture, contours, and details in cartoon images.  The input image is converted to a single-channel intensity map , where the color and luminance are removed and relative pixel intensity is preserved. Motivated by a cartoon painting method where artists firstly draw a line sketch with contours and details, and then apply color on it. A random color shift algorithm Frcs to extract single-channel texture representation from color images, which retains high-frequency textures and decreases the influence of color and luminance. Frcs(Irgb) = (1−α)(β1∗Ir+β2∗Ig+β3∗Ib)+α∗Y. Ltexture(G, Dt) = logDt(Frcs(Ic)) + log(1 − Dt(Frcs(G(Ip)))). Y represents standard grayscale image converted from RGB color image.

<br><br><br>

- In this paper, we adopt an unpaired image-to-image translation framework for image cartoonization. We decompose images into several representations, which enforces network to learn different features with separate objectives, making the learning process controllable and tunable.
- A GAN framework with a generator G and two discriminators Ds and Dt is proposed, where Ds aims to distinguish between surface representation extracted from model outputs and cartoons, and Dt is used to distinguish between texture representation extracted from outputs and cartoons.
- Pre-trained VGG network is used to extract high-level features and to impose spatial constrain on global contents between extracted structure representations and outputs, and also between input photos and outputs. Weight for each component can be adjusted in the loss function, which allows users to control the output style and adapt the model to diverse use cases.
- Ltotal = λ1 ∗ Lsurface + λ2 ∗ Ltexture + λ3 ∗ Lstructure + λ4 ∗ Lcontent + λ5 ∗ Ltv
- The total-variation loss Ltv is used to impose spatial smoothness on generated images. It also reduces highfrequency noises such as salt-and-pepper noise. Ltv = (1/(H ∗ W ∗ C))*||Delta(G(Ip)) + Delta(G(Ip))||
- The content loss Lcontent is used to ensure that the cartoonized results and input photos are semantically invariant, and the sparsity of L1 norm allows for local features to be cartoonized. Similar to the structure loss, it is calculated on pre-trained VGG16 feature space: Lcontent = kVGGn(G(Ip)) − VGGn(Ip)k.
- Iinterp = δ ∗ Fdgf (Iin, G(Iin)) + (1 − δ) ∗ G(Iin)
- We at first pre-train the generator with the content loss for 50000 iterations, and then jointly optimize the GAN based framework. Training is stopped after 100000 iterations or on convergency.
- Frechet Inception Distance summarizes the distance between the Inception feature vectors for real and generated images in the same domain. d^2 = ||mu_1 – mu_2||^2 + Tr(C_1 + C_2 – 2*sqrt(C_1*C_2)) .“mu_1” and “mu_2” refer to the feature-wise mean of the real and generated images. C_1 and C_2 are the covariance matrix for the real and generated feature vectors. Tr refers to the trace linear algebra operation, e.g. the sum of the elements along the main diagonal of the square matrix.
